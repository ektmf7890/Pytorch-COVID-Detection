{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "covid_detect.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGnnEcT1hvlW"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_sNagVyeWT-"
      },
      "source": [
        "import os\n",
        "import random\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms, models\n",
        "from torch.utils.data import DataLoader, Dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-PN17bfe_RR"
      },
      "source": [
        "class ChestXRayDataset(Dataset):\n",
        "    def __init__(self, image_dirs, transform, class_names):\n",
        "        '''\n",
        "        Parameters\n",
        "            :param image_dirs: <dict object>, key = class_names, data = directory path for each class.\n",
        "            :param transform: transform to apply to images\n",
        "            :param class_names: <list object> class names as string\n",
        "        '''\n",
        "        self.image_dirs = image_dirs\n",
        "        self.transform = transform\n",
        "        self.class_names = class_names\n",
        "\n",
        "        # images = {'covid': [], 'noraml': [], 'viral': []}\n",
        "        self.images = {}\n",
        "        \n",
        "        for class_name in class_names:\n",
        "            image_list = [x for x in os.listdir(image_dirs[class_name]) if x.lower().endswith('.png')]\n",
        "            print(f'Found {len(image_list)}images for class \\'{class_name}\\'')\n",
        "            self.images[class_name] = image_list\n",
        "    \n",
        "    def __len__(self):\n",
        "        return sum([len(self.images[class_name]) for class_name in self.class_names])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Randomly select a class\n",
        "        class_name = random.choice(self.class_names)\n",
        "\n",
        "        # Make index within range\n",
        "        index = index % len(self.images[class_name])\n",
        "\n",
        "        # Get the path of the image and open file\n",
        "        image_name = self.images[class_name][index]\n",
        "        image_path = os.path.join(self.image_dirs[class_name], image_name)\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "\n",
        "        return self.transform(image), self.class_names.index(class_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lURzW6nffVYi"
      },
      "source": [
        "# Image transformation\n",
        "image_transforms = { \n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize(size=256),\n",
        "        transforms.RandomRotation(degrees=15),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.CenterCrop(size=224),  # image size for resnet50: (224, 224)\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'valid_test': transforms.Compose([\n",
        "        transforms.Resize(size=256),\n",
        "        transforms.CenterCrop(size=224),   # image size for resnet50: (224, 224)\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yv7kY1sefiU1"
      },
      "source": [
        "# defining image directories for custom dataset\n",
        "root_dir = 'drive/MyDrive/COVID-19 Radiography Database'\n",
        "class_names = ['covid', 'normal', 'viral']\n",
        "\n",
        "train_image_dirs = {\n",
        "    'covid': os.path.join(root_dir, 'train', 'covid'),\n",
        "    'normal': os.path.join(root_dir, 'train', 'normal'),\n",
        "    'viral': os.path.join(root_dir, 'train', 'viral')\n",
        "}\n",
        "valid_image_dirs = {\n",
        "    'covid': os.path.join(root_dir, 'valid', 'covid'),\n",
        "    'normal': os.path.join(root_dir, 'valid', 'normal'),\n",
        "    'viral': os.path.join(root_dir, 'valid', 'viral')\n",
        "}\n",
        "test_image_dirs = {\n",
        "    'covid': os.path.join(root_dir, 'test', 'covid'),\n",
        "    'normal': os.path.join(root_dir, 'test', 'normal'),\n",
        "    'viral': os.path.join(root_dir, 'test', 'viral')\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAQeF4OJiI73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "860dad2a-6116-4dc2-a882-5d618255776d"
      },
      "source": [
        "# Create dataset\n",
        "train_dataset = ChestXRayDataset(\n",
        "    image_dirs=train_image_dirs, \n",
        "    transform=image_transforms['train'], \n",
        "    class_names=class_names\n",
        ")\n",
        "\n",
        "valid_dataset = ChestXRayDataset(\n",
        "    image_dirs=valid_image_dirs, \n",
        "    transform=image_transforms['valid_test'], \n",
        "    class_names=class_names\n",
        ")\n",
        "\n",
        "test_dataset = ChestXRayDataset(\n",
        "    image_dirs=test_image_dirs, \n",
        "    transform=image_transforms['valid_test'], \n",
        "    class_names=class_names\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1160images for class 'covid'\n",
            "Found 1301images for class 'normal'\n",
            "Found 1305images for class 'viral'\n",
            "Found 10images for class 'covid'\n",
            "Found 10images for class 'normal'\n",
            "Found 10images for class 'viral'\n",
            "Found 30images for class 'covid'\n",
            "Found 30images for class 'normal'\n",
            "Found 30images for class 'viral'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6EeqvIcicG4"
      },
      "source": [
        "batch_size = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4P-CyyAiSQR"
      },
      "source": [
        "# DataLoader for each Dataset\n",
        "data = {\n",
        "    'train_dataloader': DataLoader(\n",
        "        dataset=train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True\n",
        "    ),\n",
        "\n",
        "    'valid_dataloader': DataLoader(\n",
        "        dataset=valid_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True\n",
        "    ),\n",
        "\n",
        "    'test_dataloader': DataLoader(\n",
        "        dataset=test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True\n",
        "    )\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Twktvf3incq"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvzfWvjlisyn"
      },
      "source": [
        "def train_model(model, loss_func, optimizer):\n",
        "  '''\n",
        "  Parmaeters\n",
        "      :param model: Model to train and validate\n",
        "      :param loss_func: Loss function to minimize\n",
        "      :param optimizer: Optimizer for computing gradients\n",
        "      :param epochs: Number of epochs\n",
        "  Returns\n",
        "      model: Trained model\n",
        "      best_epoch: returns the index of the epoch with best accuracy\n",
        "      history: dict object, Training loss, accuracy and validation loss, accuracy\n",
        "  '''\n",
        "\n",
        "  # Get Data Loaders\n",
        "  train_dataloader = data['train_dataloader']\n",
        "  valid_dataloader = data['valid_dataloader']\n",
        "\n",
        "  # Epoch: Train + Validation\n",
        "  # Train: Forward pass -> Back propagation(get gradient) -> Update parameters -> Loss, Accuracy\n",
        "  # Validation: Forward pass -> Loss, Accuracy\n",
        "\n",
        "  # set the model to training mode\n",
        "  model.train()\n",
        "\n",
        "  # Loss and accurarcy for this epoch\n",
        "  train_loss = 0\n",
        "  train_acc = 0\n",
        "  valid_loss = 0\n",
        "  valid_acc = 0\n",
        "\n",
        "  # Train\n",
        "  for i, (inputs, labels) in enumerate(train_dataloader):\n",
        "    # inputs: 4D tensor (batch_size x 3 x width x height)\n",
        "    inputs = inputs.to(device)\n",
        "    # labels: 1D tensor (batch_size)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    # Clear existing gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    # outputs: 2D tensor (batch_size x number_of_classes)\n",
        "    outputs = model(inputs)\n",
        "\n",
        "    # Loss\n",
        "    loss = loss_func(outputs, labels)\n",
        "\n",
        "    # Backward pass: calculate gradients for parameters\n",
        "    loss.backward()\n",
        "\n",
        "    # Update parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    # Calculate loss and accuracy\n",
        "    batch_size = inputs.size(0)\n",
        "    train_loss += loss.item() * batch_size\n",
        "    \n",
        "    # predictions: 1D tensor (batch_size), class index with the largest probablility of every image in the batch\n",
        "    ret, predictions = torch.max(outputs.data, 1)\n",
        "\n",
        "    correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
        "    acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "    train_acc += acc.item() * batch_size\n",
        "\n",
        "  # Validation: No gradient checking needed\n",
        "  with torch.no_grad():\n",
        "    # set to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    for j, (inputs, labels) in enumerate(valid_dataloader):\n",
        "      # inputs: 4D tensor (bs x 3 x width x height)\n",
        "      inputs = inputs.to(device)\n",
        "      # labels: 1D tensor (bs)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      # Forward pass\n",
        "      # outputs: 2D tensor (batch_size x number_of_classes)\n",
        "      outputs = model(inputs)\n",
        "\n",
        "      # Loss\n",
        "      loss = loss_func(outputs, labels)\n",
        "\n",
        "      # Calculate loss and accuracy\n",
        "      batch_size = inputs.size(0)\n",
        "      valid_loss += loss.item() * batch_size\n",
        "\n",
        "      ret, predictions = torch.max(outputs.data, dim=1)\n",
        "      # view(shape of output), view_as(tensor whose shape is to be mimicked)\n",
        "      correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
        "      acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "      valid_acc += acc.item() * batch_size\n",
        "    \n",
        "  # Average loss and accuracy of this epoch\n",
        "  # i+1: number of batches in train set, j+1: number of batches in valid set\n",
        "  avg_train_loss = train_loss / len(train_dataloader.dataset)\n",
        "  avg_train_acc = train_acc / len(train_dataloader.dataset)\n",
        "  avg_valid_loss = valid_loss / len(valid_dataloader.dataset)\n",
        "  avg_valid_acc = valid_acc / len(valid_dataloader.dataset)\n",
        "\n",
        "  print(\"training loss {:.4f}, training accuracy {:.4f}%\\n\\tvalidation loss {:.4f}, validation accuracy {:.4f}%\".format(avg_train_loss, avg_train_acc*100, avg_valid_loss, avg_valid_acc*100))\n",
        "        \n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4i7GNp-lqyey"
      },
      "source": [
        "def test_model(model, loss_func):\n",
        "  '''\n",
        "  Function to compute the accuracy on the test set\n",
        "  Paramters\n",
        "      :param model: Model to test\n",
        "      :parar loss_func: \n",
        "      :param optimizer: Optimizer for computing gradients\n",
        "  '''\n",
        "\n",
        "  # Get DataLoader\n",
        "  test_data_loader = data['test_dataloader']\n",
        "\n",
        "  test_acc = 0.0\n",
        "  test_loss = 0.0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    # set to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    for j, (inputs, labels) in enumerate(test_data_loader):\n",
        "      # inputs: 4D tensor (bs x 3 x width x height)\n",
        "      inputs = inputs.to(device)\n",
        "      # labels: 1D tensor (bs)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      # Forward pass\n",
        "      # outputs: 2D tensor (batch_size x number_of_classes)\n",
        "      outputs = model(inputs)\n",
        "\n",
        "      # Loss\n",
        "      loss = loss_func(outputs, labels)\n",
        "\n",
        "      # Calculate loss and accuracy\n",
        "      batch_size = inputs.size(0)\n",
        "      test_loss += loss.item() * batch_size\n",
        "\n",
        "      ret, predictions = torch.max(outputs.data, dim=1)\n",
        "      correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
        "      acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "      test_acc += acc.item() * batch_size\n",
        "\n",
        "      print(\"Test Batch number: {:03d}, Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc.item()))\n",
        "  \n",
        "  # Average loss and accuracy\n",
        "  avg_test_loss = test_loss / len(test_data_loader.dataset)\n",
        "  avg_test_acc = test_acc / len(test_data_loader.dataset)\n",
        "\n",
        "  print(\"Test accuracy: {:.4f}%\".format(avg_test_acc*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6IVkyp4r9BO"
      },
      "source": [
        "# Use the pre-trained net as a feature extractor\n",
        "def tl_feature_extractor(epochs=3):\n",
        "  # load pre-trained model\n",
        "  model = models.resnet50(pretrained=True)\n",
        "\n",
        "  # freeze the exisiting parameters(exclude them from back propagation)\n",
        "  for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "  \n",
        "  # Activation: ReLU\n",
        "  # Regularization: Dropuout\n",
        "  # Final layer: Softmax\n",
        "  num_features = model.fc.in_features \n",
        "  model.fc = nn.Sequential(\n",
        "      nn.Linear(in_features=num_features, out_features=256),\n",
        "      nn.ReLU(), \n",
        "      nn.Dropout(p=0.4),\n",
        "      nn.Linear(in_features=256, out_features=10),\n",
        "      nn.LogSoftmax(dim=1) #dim=1 -> calcualte probability along row\n",
        "  )\n",
        "\n",
        "  # transfer to GPU\n",
        "  model = model.to(device)\n",
        "\n",
        "  loss_func = nn.NLLLoss()\n",
        "  # only the parameters of the fully connected layer if being updated by the optimizer\n",
        "  optimizer = optim.Adam(model.fc.parameters())\n",
        "\n",
        "  # train and validate\n",
        "  for epoch in range(epochs):\n",
        "    print('Epoch {}/{}'.format(epoch + 1, epochs))\n",
        "    train_model(model, loss_func, optimizer)\n",
        "  \n",
        "  test_model(model, loss_func)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lh25k8cYun_F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35cc2b54-936f-48a1-82b5-0921d2f80171"
      },
      "source": [
        "tl_feature_extractor(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "training loss 0.4287, training accuracy 83.1121%\n",
            "\tvalidation loss 0.3290, validation accuracy 83.3333%\n",
            "Epoch 2/10\n",
            "training loss 0.2300, training accuracy 92.1668%\n",
            "\tvalidation loss 0.1661, validation accuracy 93.3333%\n",
            "Epoch 3/10\n",
            "training loss 0.1941, training accuracy 93.1227%\n",
            "\tvalidation loss 0.1909, validation accuracy 93.3333%\n",
            "Epoch 4/10\n",
            "training loss 0.2068, training accuracy 93.0430%\n",
            "\tvalidation loss 0.1398, validation accuracy 96.6667%\n",
            "Epoch 5/10\n",
            "training loss 0.2276, training accuracy 91.2639%\n",
            "\tvalidation loss 0.1604, validation accuracy 96.6667%\n",
            "Epoch 6/10\n",
            "training loss 0.1966, training accuracy 92.9368%\n",
            "\tvalidation loss 0.3269, validation accuracy 83.3333%\n",
            "Epoch 7/10\n",
            "training loss 0.2168, training accuracy 92.2995%\n",
            "\tvalidation loss 0.1659, validation accuracy 90.0000%\n",
            "Epoch 8/10\n",
            "training loss 0.1964, training accuracy 93.1227%\n",
            "\tvalidation loss 0.3566, validation accuracy 83.3333%\n",
            "Epoch 9/10\n",
            "training loss 0.1827, training accuracy 93.2289%\n",
            "\tvalidation loss 0.3372, validation accuracy 86.6667%\n",
            "Epoch 10/10\n",
            "training loss 0.1725, training accuracy 93.7069%\n",
            "\tvalidation loss 0.0721, validation accuracy 100.0000%\n",
            "Test Batch number: 000, Loss: 0.1226, Accuracy: 0.9400\n",
            "Test Batch number: 001, Loss: 0.0590, Accuracy: 0.9750\n",
            "Test accuracy: 95.5556%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bUNIjzu1Iy6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}